<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="JPEG Pleno Objective Quality Assessment Methods for Light Field Coding Applications | ICIP 2025">
  <meta name="keywords" content="UCIP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Grand Challenge: Objective Quality Assessment Methods for Light field Coding Applications</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>

<!--
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

       <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://renyulin-f.github.io/MoE-DiffIR.github.io/">
            MoE-DiffIR
          </a>
          <a class="navbar-item" href="https://lixinustc.github.io/projects/KVQ/">
            KVQ
          </a>
           <a class="navbar-item" href="https://wxrui182.github.io/CoNo.github.io/">
            CoNo
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>
-->

<div style="background-image: url('./imgs/Anchorage_1.jpg'); background-size: cover; background-position: center; ">
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="color: black;">ICIP 2025 | Grand Challenge: Objective Quality Assessment Methods for </br> Light field Coding Applications</h1>
<!--           <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=sbiY97gAAAAJ&hl=en">Xin Li</a><sup>*1</sup>,</span>
             <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Z8PYhA4AAAAJ&hl=en">Yeying Jin</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=qHeWjNwAAAAJ&hl=en&authuser=1">Zongwei Wu</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XZugqiwAAAAJ&hl=en">Bingchen Li</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=MbVZAGQAAAAJ&hl=en">Wenhan Yang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=234Nza8AAAAJ">Yu Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1ayDJfsAAAAJ&hl=zh-CN">Zhibo Chen</a><sup>1</sup>
            </span>
             <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1ayDJfsAAAAJ&hl=zh-CN">Bihan Wen</a><sup>1</sup>
            </span>
             <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1ayDJfsAAAAJ&hl=zh-CN">Robby T. Tan</a><sup>1</sup>
            </span>
             <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1ayDJfsAAAAJ&hl=zh-CN">Radu.Timofte</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">(* Equal Contributions, ECCV 2024) </span>
        </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China,</span>
            <span class="author-block"><sup>2</sup>National University of Singapore,</span>
            <span class="author-block"><sup>3</sup>Microsoft Research Asia</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
            <!--
               <span class="link-block">
                <a href="https://codalab.lisn.upsaclay.fr/competitions/21345"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-trophy"></i>
                  </span>
                  <span>Competitions</span>
                </a>
              </span>
            !-->
              <span class="link-block">
                <a href="./Grand Challenge_Objective Quality Assessment Methods for Light field Coding Applications.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Call For Proposal</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://2025.ieeeicip.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-link"></i>
                  </span>
                  <span>ICIP 2025</span>
                </a> 
              </span>

<!--               <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a> 
              </span> -->
              <!-- Video Link. -->
              <!-- Code Link. -->
              <!--
              <span class="link-block">
                <a href="https://github.com/jinyeying/RaindropClarity"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              -->
              <!-- Dataset Link. -->
               <!--
              <span class="link-block">
                <a href="https://github.com/jinyeying/RaindropClarity"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
              -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
</div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Organizers</h2>
        <div class="content has-text-justified">
  <style>
    /* 外层容器，使用 Flex 布局，并自动换行 */
    .container {
      display: flex;
      flex-wrap: wrap;
      justify-content: center; /* 居中对齐 */
      gap: 10px;  /* 每个项目之间的间距 */
      max-width: 1000px; /* 根据需要调整总宽度 */
      margin: 0 auto;
    }
    /* 每个人的外层容器 */
    .person {
      /* 保证每行6个 */
      flex: 0 0 calc(100% / 4 - 20px);
      display: flex;
      justify-content: center;
      align-content: center;
    }
    /* 圆形背景容器 */
    .circle {
      width: 100px;       /* 可调整圆形直径 */
      height: 100px;
      background-color: #add8e6; /* 圆形背景色 */
      border-radius: 50%; /* 使容器变成圆形 */
      display: flex;
      align-items: center;
      justify-content: center;
    }
    /* 图片样式 */
    .circle img {
      width: 95%;       /* 图片宽度占圆形的 90% */
      height: 95%;      /* 图片高度占圆形的 90% */
      object-fit: cover; /* 保持图片比例，超出部分自动裁剪 */
      border-radius: 50%; /* 使图片也呈现圆形 */
    }
    .figcaption {
      font-size: 12px;
      color:black;
    }
    /* 响应式调整（例如屏幕较窄时每行显示3个） */
    @media (max-width: 760px) {
      .person {
        flex: 0 0 calc(100% / 3 - 20px);
      }
    }
  </style>
  <div class="container">
    <!-- 下面示例了12个照片，你需要将 src 替换为实际图片路径 -->
    <div class="person">
      <figure>
      <div class="circle">
        <img src="./imgs/sm2.jpg" alt="Saeed Mahmoudpour">
      </div>
        <figcaption >Saeed Mahmoudpour<br>VUB | IMEC</figcaption>
      </figure>
    </div>
    <div class="person">
       <figure>
      <div class="circle">
        <img src="./imgs/Mylene.png" alt="Mylène C.Q. Farias">
      </div>
        <figcaption >Mylène C.Q. Farias<br>TSU</figcaption>
      </figure>
    </div>
    <div class="person">
      <figure>
      <div class="circle">
        <img src="./imgs/Shengyang.jpg" alt="Shengyang Zhao">
      </div>
        <figcaption >Shengyang Zhao<br>IDT</figcaption>
      </figure>
    </div>
    <div class="person">
        <figure>
      <div class="circle">
        <img src="./imgs/Peter_Schelkens.jpg" alt="Peter Schelkens">
      </div>
        <figcaption >Peter Schelkens<br>VUB | IMEC <br><br></figcaption>
      </figure>
    </div>
  </div>
      
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Grand Challenge Background</h2>
        <div class="content has-text-justified">
          <p>Light field imaging has emerged as a transformative technology for capturing andvisualizing three-dimensional scenes. By recording spatial and angular information, light fields enable advanced applications in immersive media, virtual reality, computational photography, and scientific imaging. However, due to the massive amount of data generated, efficient compression techniques are essential for enabling practical use and widespread adoption of light field technology. </p> 

          <p> Compression algorithms often introduce distortions, which can degrade visual quality and thus, assessing the visual impact of distortions is critical to ensuring that compressed light fields maintain their perceptual fidelity. The subjective quality assessment relies on human observers to assess perceptual quality. While these methods are highly reliable, they are time-consuming and impractical for large-scale or iterative testing scenarios. To overcome these limitations, objective quality assessment metrics have gained importance as they offer an automated, scalable, and repeatable approach to evaluating visual quality. </p>
          
          <p>This Grand Challenge seeks proposals for <b style="color: red;">full-reference</b> and <b style="color: red;">no-reference</b> objective light field quality assessment methodologies to evaluate perceptual quality in light field coding applications. These methodologies will be tested on subjectively annotated light field datasets to ensure alignment with human opinions. </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Why this Grand Challenge?</h2>
        <div class="content has-text-justified">
          <p>A reliable framework for objective quality assessment accelerates innovation by providing consistent benchmarks for evaluating new compression techniques. This facilitates the development of efficient and perceptually optimized codecs, enabling more practical use of light field technology in consumer and professional applications. Objective metrics reduce the dependency on extensive subjective testing, offering faster and more cost-effective evaluation methods while ensuring scalability. </p>
          <p>The JPEG Pleno initiative supports these advancements by driving standardization efforts, including the integration of subjective and objective quality assessment methods for plenoptic modalities in the new Part 7 Quality Assessment of the JPEG Pleno standard. Contributors to this Grand Challenge will have the opportunity to collaborate on the ongoing standardization activities, helping to ensure the practical application of their methodologies.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Rules for Participation</h2>
        <div class="content has-text-justified">
          <p>Participants are invited to submit proposals for full-reference and/or no-reference objective light field quality assessment methods, following these rules:</p>
          <ul>
            <li>Each submission must include: 
              <ul>
                <li>A summary of the proposed methodology.</li>
                <li>A platform-independent executable of the algorithm must be provided, including command-line parameters for evaluation. The executable should support directories (containing light fields of n by m views) and be capable of reading .ppm and .png file formats. An example of such datasets will be shared before the deadline.</li>
                <li>Information about the training dataset and process used in the development of the objective method, when applicable.</li>
                <li>For <b><i>full-reference submission track: </i></b>the command line must support at least two input directories: one for the reference light fields and one for the test light fields. An example of the command line:</li>
                <code>./executable --reference_dir /path_to_ref_lightfields --test_dir /path_to_test_lightfields</code>
                <li>For <b><i>no-reference submission track: </i></b>the command line must support at least one input directory for the test light fields. For example:</li>
                <code>./executable --test_dir /path_to_test_lightfields</code>
              </ul>
            </li>
            <li>Paper submission on the proposed objective quality metric is not mandatory but encouraged. Proponents shall present their methods at the Special Session organized at ICIP 2025.</li>
          </ul>

          <p>A sample dataset will be provided to the participants, who must ensure that their code functions correctly with these sample datasets. Additionally, participants should be readily available to quickly address any issues that arise when running their executable.</p>
          <p>The output of executing the command line should provide a quality score.</p>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluation Process</h2>
        <div class="content has-text-justified">
          <p>Two competition tracks will be organized for full-reference and no-reference methods. The proposed methodologies will be evaluated for their alignment with subjective quality scores based on the following quantitative criteria:</p>
          <ul>
            <li><b>Correlation Metrics</b> Pearson Linear Correlation Coefficient (PLCC) and Spearman Rank Order Correlation Coefficient (SROCC) will measure the correlation between objective and subjective scores.</li>
            <li><b>Root Mean Square Error (RMSE):</b> RMSE will be computed between objective and subjective scores after mapping the former to the latter.</li>
          </ul>
          <p>The methodology achieving the best overall performance across these metrics will be declared the winner.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Important dates</h2>
        <div class="content has-text-justified">
           <ul>
          <li><strong>2025.07.18</strong> Deadline for registration of proposals. </li>
          <li><strong>2025.08.25</strong> Deadline for submission of proposals.</li>
           <li><strong>2025.08.26</strong> Releasing test dataset for cross-validation of the produced objective scores.</li>
          <li><strong>2025.09</strong> Special session for presentation of the proposals and winner announcement.</li>
           </ul>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Datasets</h2>
        <div class="content has-text-justified">
          <p>The organizers have created a subjectively annotated dataset based on extensive subjective experiments with light fields of varying resolutions and baselines, including both dense and sparse representations. The dataset comprises natural and synthetic light fields compressed using state-of-the-art encoding techniques across multiple bitrates.</p>
          <p>Proponents are required to submit the proposal requirements (summarized in section C) by the specified deadline. The organizing committee will compute the scores and evaluate their performance against the annotated dataset. After the deadline, the test dataset will be made publicly available for cross-validation.</p>
        </div>
      </div>
    </div>
  </div>
</section>
 


<!--
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" autoplay muted loop playsinline height="100%">
            <img src="./imgs/RaindropClarity.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
      </img>
      <h2 class="subtitle has-text-centered">
        Raindrop Dataset
      </h2>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
     <pre><code>
@inproceedings{ntire2025day,
  title={{NTIRE} 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results},
  author={Xin Li and Yeying Jin and Xin Jin and Zongwei Wu and Bingchen Li and Yufei Wang and
	Wenhan Yang and Yu Li and Zhibo Chen and Bihan Wen and Robby Tan and Radu Timofte
      	and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year={2025}
}
}
    </code></pre>
    <pre><code>
  @inproceedings{jin2024raindrop,
  title={Raindrop Clarity: A Dual-Focused Dataset for Day and Night Raindrop Removal},
  author={Jin, Yeying and Li, Xin and Wang, Jiadong and Zhang, Yan and Zhang, Malu},
  booktitle={European Conference on Computer Vision},
  pages={1--17},
  year={2024},
  organization={Springer}
}
    </code></pre>
  </div>
</section>

-->





<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/lixinustc" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
