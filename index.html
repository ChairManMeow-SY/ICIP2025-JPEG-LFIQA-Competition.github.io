<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="JPEG Pleno Objective Quality Assessment Methods for Light Field Coding Applications | ICIP 2025">
  <meta name="keywords" content="UCIP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Grand Challenge: Objective Quality Assessment Methods for Light field Coding Applications</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

<!--       <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://renyulin-f.github.io/MoE-DiffIR.github.io/">
            MoE-DiffIR
          </a>
          <a class="navbar-item" href="https://lixinustc.github.io/projects/KVQ/">
            KVQ
          </a>
           <a class="navbar-item" href="https://wxrui182.github.io/CoNo.github.io/">
            CoNo
          </a>
        </div>
      </div> -->
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ICIP 2025 | Grand Challenge: Objective Quality Assessment Methods for Light field Coding Applications</h1>
<!--           <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=sbiY97gAAAAJ&hl=en">Xin Li</a><sup>*1</sup>,</span>
             <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Z8PYhA4AAAAJ&hl=en">Yeying Jin</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=qHeWjNwAAAAJ&hl=en&authuser=1">Zongwei Wu</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XZugqiwAAAAJ&hl=en">Bingchen Li</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=MbVZAGQAAAAJ&hl=en">Wenhan Yang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=234Nza8AAAAJ">Yu Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1ayDJfsAAAAJ&hl=zh-CN">Zhibo Chen</a><sup>1</sup>
            </span>
             <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1ayDJfsAAAAJ&hl=zh-CN">Bihan Wen</a><sup>1</sup>
            </span>
             <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1ayDJfsAAAAJ&hl=zh-CN">Robby T. Tan</a><sup>1</sup>
            </span>
             <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1ayDJfsAAAAJ&hl=zh-CN">Radu.Timofte</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">(* Equal Contributions, ECCV 2024) </span>
        </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China,</span>
            <span class="author-block"><sup>2</sup>National University of Singapore,</span>
            <span class="author-block"><sup>3</sup>Microsoft Research Asia</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
            <!--
               <span class="link-block">
                <a href="https://codalab.lisn.upsaclay.fr/competitions/21345"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-trophy"></i>
                  </span>
                  <span>Competitions</span>
                </a>
              </span>
            !-->
              <span class="link-block">
                <a href="./Grand Challenge_Objective Quality Assessment Methods for Light field Coding Applications.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Call For Proposal</span>
                </a>
              </span>
<!--               <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a> -->
              </span>
              <!-- Video Link. -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/jinyeying/RaindropClarity"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/jinyeying/RaindropClarity"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h1 class="title is-3">Organizers</h2>
        <div class="content has-text-justified">
          <p>
  <style>
    /* 外层容器，使用 Flex 布局，并自动换行 */
    .container {
      display: flex;
      flex-wrap: wrap;
      justify-content: center; /* 居中对齐 */
      gap: 10px;  /* 每个项目之间的间距 */
      max-width: 1000px; /* 根据需要调整总宽度 */
      margin: 0 auto;
    }
    /* 每个人的外层容器 */
    .person {
      /* 保证每行6个 */
      flex: 0 0 calc(100% / 6 - 20px);
      display: flex;
      justify-content: center;
    }
    /* 圆形背景容器 */
    .circle {
      width: 100px;       /* 可调整圆形直径 */
      height: 100px;
      background-color: #add8e6; /* 圆形背景色 */
      border-radius: 50%; /* 使容器变成圆形 */
      display: flex;
      align-items: center;
      justify-content: center;
    }
    /* 图片样式 */
    .circle img {
      width: 95%;       /* 图片宽度占圆形的 90% */
      height: 95%;      /* 图片高度占圆形的 90% */
      object-fit: cover; /* 保持图片比例，超出部分自动裁剪 */
      border-radius: 50%; /* 使图片也呈现圆形 */
    }
    /* 响应式调整（例如屏幕较窄时每行显示3个） */
    @media (max-width: 800px) {
      .person {
        flex: 0 0 calc(100% / 2 - 20px);
      }
    }
  </style>
  <div class="container">
    <!-- 下面示例了12个照片，你需要将 src 替换为实际图片路径 -->
    <div class="person">
      <figure>
      <div class="circle">
        <img src="./imgs/sm2.jpg" alt="Saeed Mahmoudpour">
      </div>
        <figcaption style="font-size: 8px;">Saeed Mahmoudpour<br>VUB</figcaption>
      </figure>
    </div>
    <div class="person">
       <figure>
      <div class="circle">
        <img src="./imgs/Mylene.png" alt="Mylène C.Q. Farias">
      </div>
        <figcaption style="font-size: 8px;">Mylène C.Q. Farias<br>TSU</figcaption>
      </figure>
    </div>
    <div class="person">
      <figure>
      <div class="circle">
        <img src="./imgs/Shengyang.jpg" alt="Shengyang Zhao">
      </div>
        <figcaption style="font-size: 8px;">Shengyang Zhao<br>IDT</figcaption>
      </figure>
    </div>
    <div class="person">
        <figure>
      <div class="circle">
        <img src="./imgs/Peter_Schelkens.jpg" alt="Peter Schelkens">
      </div>
        <figcaption style="font-size: 8px;">Peter Schelkens<br>VUB | IMEC </figcaption>
      </figure>
    </div>
  </div>
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Grand Challenge Background</h2>
        <div class="content has-text-justified">
          <p>Light field imaging has emerged as a transformative technology for capturing andvisualizing three-dimensional scenes. By recording spatial and angular information, light fields enable advanced applications in immersive media, virtual reality, computational photography, and scientific imaging. However, due to the massive amount of data generated, efficient compression techniques are essential for enabling practical use and widespread adoption of light field technology. </p> 

          <p> Compression algorithms often introduce distortions, which can degrade visual quality and thus, assessing the visual impact of distortions is critical to ensuring that compressed light fields maintain their perceptual fidelity. The subjective quality assessment relies on human observers to assess perceptual quality. While these methods are highly reliable, they are time-consuming and impractical for large-scale or iterative testing scenarios. To overcome these limitations, objective quality assessment metrics have gained importance as they offer an automated, scalable, and repeatable approach to evaluating visual quality. </p>
          
          <p>
This Grand Challenge seeks proposals for full-reference and no-reference objective
light field quality assessment methodologies to evaluate perceptual quality in light field
coding applications. These methodologies will be tested on subjectively annotated light
field datasets to ensure alignment with human opinions.
          <p> Raindrops on camera lenses or windshields significantly reduce image visibility in human life, posing challenges for applications like surveillance and autonomous driving. Effective raindrop removal is crucial to ensuring reliable performance in these systems. However, most deraining datasets overlooked the complex environment in real life. In particular, there are few raindrop-focused datasets, and most existing datasets focus on capturing background scenes while the camera is focused on the background. Meanwhile, these datasets primarily target daytime scenarios, with limited attention to nighttime conditions.</p>

          <p>To promote the development of deraining, we introduce the first large-scale real-world raindrop-clarity dataset, which includes raindrop-focused images and nighttime scenes. Based on this dataset, we have the first NTIRE Challenge on Day and Night Raindrop Removal for Dual-Focused Images, jointly with the NTIRE workshop.</p>

          <p>The goal of this competition is to establish a new and applicable benchmark for the Day and Night Raindrop Removal for Dual-Focused Images. We are looking forward to the collaborative efforts of our participants, aiming to elevate the quality of training images. The final ranking will be made based on PSNR, SSIM, and LPIPS. </p>

          <p>The top-ranked participants will be awarded and invited to follow the CVPR submission guide for workshops to describe their solution and to submit to the associated NTIRE workshop at CVPR 2025.</p>

        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Important dates</h2>
        <div class="content has-text-justified">
           <ul>
        <li><strong>2025.01.28</strong> Release of train data (input and output) </li>
           <li><strong>2025.02.01</strong> Release of validation data (inputs only) </li>
           <li><strong>2025.02.01</strong> Validation server online </li>
          <li><strong>2025.03.15</strong> Final test data release (inputs only)</li>
            <li><strong>2025.03.21</strong> Test output results submission deadline</li>
          <li><strong>2025.03.22</strong> Fact sheets and code/executable submission deadline</li>
          <li><strong>2025.03.24</strong> Preliminary test results release to the participants</li>
           <li><strong>2025.03.28</strong> Paper submission deadline for entries from the challenge</li>
          <li><strong>2025.06.17</strong> NTIRE workshop and challenges, results, and award ceremony (CVPR 2025, Nashville, USA)</li>
           </ul>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
 
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Data</h2>
        <div class="content has-text-justified">
          <p> The dataset named RainDrop Clarity is contributed by Yeying Jin et al (NUS). The dataset includes both daytime and nighttime scenes for training and testing. (i) The daytime raindrop dataset contains a total of 5,442 paired or triplet images, with 4,713 pairs used for training and the remaining 729 pairs for testing. (ii) The nighttime raindrop dataset consists of 9,744 paired or triplet images, where 8,655 pairs are allocated to the training set, and the remaining 1,089 pairs are reserved for the validation and test set.</p>
          <p> In this competition, the used dataset is composed of: 
                  <li>Train data: A total of 14,139 day and night dual-focused raindrop images with blurry backgrounds and their corresponding clear backgrounds are provided.</li>
                  <li>Validation data: A total of 240 day and night rainy images are provided.</li> 
                  <li>Test data: A total of 731 day and night rainy images are provided</li>
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>




<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" autoplay muted loop playsinline height="100%">
            <img src="./imgs/RaindropClarity.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
      </img>
      <h2 class="subtitle has-text-centered">
        Raindrop Dataset
      </h2>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
     <pre><code>
@inproceedings{ntire2025day,
  title={{NTIRE} 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results},
  author={Xin Li and Yeying Jin and Xin Jin and Zongwei Wu and Bingchen Li and Yufei Wang and
	Wenhan Yang and Yu Li and Zhibo Chen and Bihan Wen and Robby Tan and Radu Timofte
      	and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year={2025}
}
}
    </code></pre>
    <pre><code>
  @inproceedings{jin2024raindrop,
  title={Raindrop Clarity: A Dual-Focused Dataset for Day and Night Raindrop Removal},
  author={Jin, Yeying and Li, Xin and Wang, Jiadong and Zhang, Yan and Zhang, Malu},
  booktitle={European Conference on Computer Vision},
  pages={1--17},
  year={2024},
  organization={Springer}
}
    </code></pre>
  </div>
</section>







<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/lixinustc" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
